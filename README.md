# multi_clip_gan
# 🧠 Enhancing Image Captioning Using MultiCLIP-GAN Integration

This project focuses on generating accurate and contextually relevant image captions by enhancing low-quality images using RealESRGAN and generating captions using multimodal models like CLIP and BLIP.

---

## 📁 Dataset Location

The dataset is too large to upload to GitHub.  
It is stored in **Google Drive** and can be accessed here:

🔗 [Click here to access the dataset](https://drive.google.com/drive/folders/1HfiXeMapnkWPBbBp7VmCkLmsXcGrLcY_?usp=drive_link)

---

## 🚀 Technologies Used

- Python
- Google Colab
- PyTorch / TensorFlow
- CLIP (Contrastive Language–Image Pre-training)
- BLIP
- Real-ESRGAN (for image enhancement)

---

## 📦 Files in This Repo

- `README.md` – Project info

---

## 🔧 How to Run (on Colab)

1. Clone this repo:
   ```bash
   git clone https://github.com/Petchikutti/multi_clip_gan.git
