# multi_clip_gan
# ğŸ§  Enhancing Image Captioning Using MultiCLIP-GAN Integration

This project focuses on generating accurate and contextually relevant image captions by enhancing low-quality images using GANs and generating captions using multimodal models like CLIP and BLIP.

---

## ğŸ“ Dataset Location

The dataset is too large to upload to GitHub.  
It is stored in **Google Drive** and can be accessed here:

ğŸ”— [Click here to access the dataset](https://drive.google.com/your-dataset-link)

(Replace the above link with your actual sharable Google Drive link)

---

## ğŸš€ Technologies Used

- Python
- Google Colab
- PyTorch / TensorFlow
- CLIP (Contrastive Languageâ€“Image Pre-training)
- BLIP
- Real-ESRGAN (for image enhancement)

---

## ğŸ“¦ Files in This Repo

- `README.md` â€“ Project info

---

## ğŸ”§ How to Run (on Colab)

1. Clone this repo:
   ```bash
   git clone https://github.com/Petchikutti/multi_clip_gan.git
